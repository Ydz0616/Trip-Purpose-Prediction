\documentclass[11pt]{article}
\usepackage{fullpage}
\usepackage{times}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{url}
\usepackage{hyperref}
\usepackage{float}

\begin{document}

\noindent\rule{\textwidth}{0.6pt}

\begin{center}
{\LARGE \textbf{Project Report}}\\[4pt]
{\large \textbf{Second Draft}}\\[8pt]
{\normalsize \textbf{Group 45:} Ajay Partha, Albert Ding, Bhrugu Bharathi, Yuandong Zhang}
\end{center}

\noindent\rule{\textwidth}{0.6pt}

\vspace{0.6cm}

\section{Problem Description}

This project studies how a traveler’s latent trip purposes evolve with time and how they can be used to identify travel behavior. Our key assumption is that the purpose behind each trip (“shopping”, “leisure”, etc.) is not directly observed but the trip’s mode of transportation (“walking”, “bus”, “bike”, etc.) is observed. 

The goal is to:
\begin{itemize}
    \item Train a Markov Model using the technique, Maximum Likelihood Estimation (MLE) using observed modes.The training data that we have collected contains true purpose labels, meaning we can directly calculate the transition probabilities between purposes and the emission probabilities from purpose using frequency counts.
    \item Use a Hidden Markov Model structure during testing by applying the Viterbi algorithm, to identify the most likely sequence of latent purposes. After we have estimated the transition and emission matrix using MLE during training, we can treat the test set as hidden by removing the purpose labels and only providing the mode sequence. The Viterbi algorithm can now be applied to decode the most likely sequence of latent purposes.
    \item Evaluate the learned hidden states against the true purposes in the Geolife dataset to see if they correlate.
\end{itemize}

Our problem can be vital in understanding human mobility patterns and activity sequences, which can then be used in informing transportation policies or even personalized services. Due to the fact that the true purposes are hidden and evolve over time in a Markovian way, an HMM structure is perfect for this situation. But, since everything for training is observed, we will not use EM but MLE estimation for transition and emission parameters.

\section{Data Sourcing and Processing}

Our data comes from the Microsoft Research Asia Geolife GPS Trajectory Dataset, which contains GPS traces from over 100 users in Beijing from April 2007 to August 2012. The dataset shows GPS points (latitude and longitude) and pre-annotated trip metadata such as \texttt{trip\_mode}, \texttt{start\_purpose}, and \texttt{end\_purpose}. In the HMM we train, only the mode of travel serves as our observed variable and the purpose is only the \texttt{end\_purpose} which we treat as hidden, used only for evaluation purposes.

There were four main preprocessing steps to ensure our HMM model worked properly on the dataset:

\begin{enumerate}
    \item First, we grouped by \texttt{trip\_id}, and sorted by timestamp to reconstruct full trip trajectories from GPS datapoints. This allowed us to reconstruct trips and calculate features like duration in a much easier fashion. HMMs require ordered sequences which is also why we had to complete this preprocessing step.

    \item Secondly, we computed time differences between every pair of consecutive GPS points. Computing inter-point time deltas allowed us to validate trip continuity, detect gaps, and verify that a sequence of points corresponded to a continuous trip. Trips that had missing data or irregular sampling could distort our ordering which would then affect the state transitions in the HMM.

    \item Third, after reconstructing trips, we grouped by user id and trip date to obtain the users’ daily trips, and we sorted the daily trips w.r.t the timestamps, to create a behavioral sequence for the HMM.

    \item Finally, we filtered low-quality data. We removed users with fewer than 10 total trips, daily sequences with fewer than 3 trips, and trips shorter than 5 minutes. This is important because sequences that are too short don’t provide informative transitions for a training HMM model and very short trips often correspond to noise rather than meaningful human activities.
\end{enumerate}

Our final dataset contains 1,158,825 entries. Each entry represents a single trip made by a user, including the transportation mode, the trip purpose, and an index indicating its position within the user’s daily sequence.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Screenshot 2025-12-01 at 12.37.18 PM.png}
    \caption{Distribution of trip mode and trip purpose in GeoLife data}
    \label{fig:placeholder}
\end{figure}

These models help us understand why some modes could be more informative than others during the decoding process.

\section{Modeling and Inference}

We model trip behavior using a discrete-emission Hidden Markov Model (HMM) for our project. However, we make a very critical distinction between our training and testing phase. During training, all of our purposes are observed so we can directly estimate the model parameters using MLE. On the other hand during testing, the purposes are hidden, so we decide to apply the Viterbi algorithm to decode the most likely sequence of latent purposes.

\vspace{0.4cm}
\noindent\textbf{Hidden state at trip t:}
\[
Z_t \in \{1, \dots, K\}
\]
This is our latent “purpose scheme” (leisure, work, etc.).

\vspace{0.4cm}
\noindent\textbf{Observation at trip t:}
\[
O_t \in \{1, \dots, M\}
\]
This is the trip\_mode in our dataset (mode of travel).

\vspace{0.6cm}
\noindent\textbf{Our parameters can be summarized as follows:}

\vspace{0.6cm}
\noindent\textbf{Initial state distribution} \quad $(\pi \in \mathbb{R}^K)$
\begin{itemize}
    \item $\pi_i$ is the probability the day begins with purpose $i$ (e.g., leisure, shopping).
    \item Training data includes the true purposes so we can estimate each $\pi_i$ with frequency counts.
\[
\pi_i = P(Z_1 = i)
\]
\[
\pi_i = \frac{\#\text{days whose first trip has purpose } i}{\#\text{total days}}
\]
\end{itemize}

\vspace{0.6cm}
\noindent\textbf{Transition matrix} \quad $(A \in \mathbb{R}^{K \times K})$
\begin{itemize}
    \item $A$ is a $K \times K$ matrix.
    \item $A(i,j)$ represents $P(Z_{t+1} = j \mid Z_t = i)$ which is the probability of switching from purpose $i$ to purpose $j$ from one trip to the next.
\[
A_{ij} = P(Z_{t+1} = j \mid Z_t = i)
\]
\[
A_{ij} = \frac{\#(Z_t = i,\, Z_{t+1} = j)}{\#(Z_t = i)}
\]
    \item In our data these transitions relate to the behavioral patterns across trips (e.g., work $\rightarrow$ home).
\end{itemize}

\vspace{0.6cm}
\noindent\textbf{Emission matrix} \quad $(B \in \mathbb{R}^{K \times M})$
\begin{itemize}
    \item $B$ is a $K \times M$ matrix.
    \item $B(i,k)$ represents $P(O_t = k \mid Z_t = i)$ which is the probability of observing mode $k$ when in the purpose state $i$.
\[
B_{ik} = P(O_t = k \mid Z_t = i)
\]
\[
B_{ik} = \frac{\#(Z_t = i,\, O_t = k)}{\#(Z_t = i)}
\]
\end{itemize}

\vspace{0.6cm}
We assume that $P(Z_{t+1} \mid Z_{1:t}) = P(Z_{t+1} \mid Z_t)$ given the Markov dynamics over latent purposes. Also, emissions only depend on the current latent purpose, meaning $P(O_t \mid Z_{1:t}, O_{1:t-1}) = P(O_t \mid Z_t)$.

\vspace{0.6cm}
Every day starts in a latent purpose that was selected according to an initial state distribution, which defines how likely each purpose is to happen at the start of a sequence. As the user (found through user\_id) goes through their day, the hidden purpose changes due to a transition probability matrix that figures out the likeliness of a purpose to be followed by another. This could be how likely a work-purpose trip is followed by a leisure-purpose trip or a home-purpose trip. Then given a latent purpose state at time $t$, the observed mode at the time is drawn based on an emission distribution, which shows how likely travel modes are for each purpose.

\vspace{0.6cm}
We utilize MLE to estimate the initial, transition, and emission probabilities using frequency counts since everything is observed during training. Then during testing we hide the purposes, allowing us to apply the Viterbi algorithm to this created HMM and utilize the matrices found during training to inform how the decoder weighs staying in the same purpose or switching to another.


\section{Baselines}

\subsection*{Rule-Based}

The idea of this is certain trip purposes follow very predictable daily patterns. For example, “work” is always in the morning, and “home” is always during the evening. We can use the timestamp of each trip and make rules such as:
\begin{itemize}
    \item if time is between 7--10am, predict work,
    \item if time is between 8--10pm, predict home.
\end{itemize}

This baseline is very simple so it can help provide us a lower bound with what our model should perform better than.

\subsection*{Probability-Based}

The probability-based model would count the number of each purpose given an observation in the dataset to create a probability distribution of purposes for each observed state. So from the training set we compute:
\[
P(\text{purpose} = p \mid \text{mode} = m)
\]
Then for each observed mode, we just predict the purpose with the highest conditional probability. This ignores temporal structure, treating all trips independently.



\subsection*{Random Forest Baseline}

This is a non-sequential machine learning baseline using a Random Forest classifier. Each trip is treated independently where the inputs include observed attributes such as mode of transportation, and the output label is the trip purpose.

This is how the Random forest classifier is trained on trip data:
\begin{itemize}
    \item \textbf{Training:} For each observed trip, we construct a feature vector and use the true purpose as the target label. The Random Forest learns a set of decision trees that separate feature space into sections corresponding to different purposes.
    \item \textbf{Prediction:} At test time, for each observed mode, the Random Forest predicts a purpose by calculating votes across trees.
\end{itemize}

This baseline is more flexible than the rule-based and pure frequency-based baselines. Comparing our HMM against this Random Forest baseline can help us understand the additional value coming from modeling temporal dependencies.

\section{Models}
\subsection*{HMM}

placeholder

\subsection*{N-gram model}

placeholder

\subsection*{Edge-Emitting HMM}

Instead of emitting an observation from a state, the transition itself emits the mode:
\[
P(O_t \mid Z_{t-1}, Z_t)
\]
This captures sequences of events instead of individual states. This puts more emphasis on behavioral patterns. Some behavior patterns are better described as pairs.

\section{Results and Discussion}

We use a binary accuracy function to measure the model’s performance, where the predicted purpose is compared to the ground truth purpose with an indicator function.
\begin{table}[h!]
\centering
\begin{tabular}{c|ccc}
    & \textbf{Rule-based} & \textbf{Probability-based} & \textbf{Random Forest} \\
    \hline
    \textbf{Accuracy} & 0.2092 & 0.2722 & 0.4617 \\
\end{tabular}
\end{table}

So far we've included three baselines, rule-based, which predicts purposes given time of day; probability-based, which predicts purpose by random sampling given mode and its purpose distribution as weights, and random forest, which also ignore the sequential nature of the dataset and treats every trip as one training entry, and predicts a purpose given mode.

Among the baselines, the rule-based model has the lowest accuracy of 0.2092, which is almost close to the probability randomly guessing one of the five trip purposes. Probability-based baseline also suffers from lack of sequential understanding of the trip sequences. Random forest model, which also ignores sequential information, has the highest accuracy of 0.4617. However, by looking at its confusion matrix, we've discovered the imbalanced dataset has caused this model to give better predictions on entries it seen more and vice versa.



\section{Conclusion}

Our results (will hopefully) show that maximum-likelihood Markov Models combined with a Viterbi decoding can recover behavioral patterns from transportation mode sequences.

Evaluating our model against ground-truth trip purposes, the learned hidden states show…\\
Increasing the number of latent states did…

Some limitations to our model is that mode observations aren't the strongest evidence for the purpose of trip and noise in GPS-derived trip segmentation could lead to difficulties with the model differentiating similar activities. Future extensions of this could be exploring a variable-order Markov model which allows for a more expressive sequential model. We could also incorporate contextual features such as the time of day, or duration of the event. Overall, our work really showed that this probabilistic model can extract the latent pattern in mobility behavior.

\section{Reflections \& Contributions}

(Add your section later.)

\section{References}

Zheng, Y., Fu, H., Xie, X., Ma, W.-Y., \& Li, Q. (2011). \textit{Geolife GPS Trajectory Dataset – User Guide}.

https://www.microsoft.com/en-us/research/publication/geolife-gps-trajectory-dataset-user-guide/

\end{document}
